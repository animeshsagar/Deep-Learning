{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xV_TZ7TnxGBw"
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "qhmNOZg8bdpb",
    "outputId": "659f4f4f-b7bd-4da3-abbb-494e0b160c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.3.1)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
      "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.16.5)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.21.3)\n",
      "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
      "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.29.0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from google.colab import files\n",
    "import numpy as np\n",
    "!pip install librosa\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhS_a4uixA4i"
   },
   "source": [
    "# Mount Google Drive for file support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EilvDK3KxLoV",
    "outputId": "155d313d-e1fc-413a-edee-32912b72bc03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYh5vU4Kutu7"
   },
   "source": [
    "I have mounted my google drive to use the wav files from it. Please change the path to local directory in order to run the code and download the files in the current working directory.\n",
    "\n",
    "Anopther Option is to uncomment the upload button code and upload files individually from the local system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_ejyu5Xvbo-"
   },
   "source": [
    "# Load & Convert Wav files to respective Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3P_Ck8rbolN"
   },
   "outputs": [],
   "source": [
    "#S and X are two matrices of size 513x2459  Short time fourier Transform\n",
    "s, sr=librosa.load('/content/gdrive/My Drive/train_clean_male.wav', sr=None)\n",
    "S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "sn, sr=librosa.load('/content/gdrive/My Drive/train_dirty_male.wav', sr=None)\n",
    "X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "\n",
    "s1, sr=librosa.load('/content/gdrive/My Drive/test_x_01.wav', sr=None)\n",
    "S1 =librosa.stft(s1, n_fft=1024, hop_length=512)\n",
    "s2, sr=librosa.load('/content/gdrive/My Drive/test_x_02.wav', sr=None)\n",
    "S2 =librosa.stft(s2, n_fft=1024, hop_length=512)\n",
    "# print(S1.shape, S2.shape)\n",
    "# print(S.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oca0_fndvqtm"
   },
   "source": [
    "# Calculate Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvloFYyybq7c"
   },
   "outputs": [],
   "source": [
    "S_abs=np.transpose(np.abs(S))\n",
    "X_abs=np.transpose(np.abs(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XVGsCOcdtWW"
   },
   "outputs": [],
   "source": [
    "#Declaring Placeholder Variables\n",
    "x = tf.placeholder(tf.float32, [None, 513])\n",
    "y = tf.placeholder(tf.float32, [None, 513])\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5yHDStlwB3W"
   },
   "source": [
    "# Initialise Weights & Biases for the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Szf_sSeTjC8r"
   },
   "outputs": [],
   "source": [
    "# W1 = tf.Variable(tf.random_normal([513, 1024], stddev=0.03), name='W1')\n",
    "# b1 = tf.Variable(tf.random_normal([1024]), name='b1')\n",
    "# # and the weights connecting the hidden layer to the output layer\n",
    "# W2 = tf.Variable(tf.random_normal([1024, 1024], stddev=0.03), name='W2')\n",
    "# b2 = tf.Variable(tf.random_normal([1024]), name='b2')\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([1024, 513], stddev=0.03), name='W3')\n",
    "# b3 = tf.Variable(tf.random_normal([513]), name='b3')\n",
    "\n",
    "W1=tf.Variable(tf.initializers.he_normal(seed=None)((513, 1024)))\n",
    "b1 = tf.Variable(tf.random_normal([1024]), name='b1')\n",
    "\n",
    "W2=tf.Variable(tf.initializers.he_normal(seed=None)((1024, 1000)))\n",
    "b2 = tf.Variable(tf.random_normal([1000]), name='b2')\n",
    "\n",
    "W3=tf.Variable(tf.initializers.he_normal(seed=None)((1000, 1024)))\n",
    "b3 = tf.Variable(tf.random_normal([1024]), name='b3')\n",
    "\n",
    "W4=tf.Variable(tf.initializers.he_normal(seed=None)((1024, 513)))\n",
    "b4 = tf.Variable(tf.random_normal([513]), name='b4')\n",
    "\n",
    "# W4 = tf.Variable(tf.random_normal([1024, 513], stddev=0.03), name='W4')\n",
    "# b4 = tf.Variable(tf.random_normal([513]), name='b4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHM5ZMwuwIS-"
   },
   "source": [
    "# Compose the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yayKu--FjCl7"
   },
   "outputs": [],
   "source": [
    "#Putting together Layers\n",
    "hidden_1 = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_1 = tf.nn.relu(hidden_1)\n",
    "hidden_2 = tf.add(tf.matmul(hidden_1, W2), b2)\n",
    "hidden_2 = tf.nn.relu(hidden_2)\n",
    "hidden_3 = tf.add(tf.matmul(hidden_2, W3), b3)\n",
    "hidden_3 = tf.nn.relu(hidden_3)\n",
    "\n",
    "\n",
    "y_ = tf.nn.relu(tf.add(tf.matmul(hidden_3, W4), b4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LN4KVKylwRQA"
   },
   "source": [
    "# Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3clQWm_eMAE"
   },
   "outputs": [],
   "source": [
    "cost= tf.losses.mean_squared_error(y,y_, weights=1.0)\n",
    "loss_function = tf.reduce_mean(cost, name = 'loss_function')\n",
    "optimize = tf.train.AdamOptimizer(0.0001).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmr6nXs_wXED"
   },
   "source": [
    "# Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EvO-zhDWlyaY",
    "outputId": "7b5181ea-3f5c-45a0-8cdd-a819e28c358d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 loss:  9.045808881521225\n",
      "Epoch  1 loss:  1.8065841645002365\n",
      "Epoch  2 loss:  0.8350165784358978\n",
      "Epoch  3 loss:  0.7110746577382088\n",
      "Epoch  4 loss:  0.6367659345269203\n",
      "Epoch  5 loss:  0.5752289034426212\n",
      "Epoch  6 loss:  0.5268498286604881\n",
      "Epoch  7 loss:  0.4872427601367235\n",
      "Epoch  8 loss:  0.4549154043197632\n",
      "Epoch  9 loss:  0.42887023463845253\n",
      "Epoch  10 loss:  0.40779533982276917\n",
      "Epoch  11 loss:  0.39040587842464447\n",
      "Epoch  12 loss:  0.37628900073468685\n",
      "Epoch  13 loss:  0.3647633586078882\n",
      "Epoch  14 loss:  0.35527439415454865\n",
      "Epoch  15 loss:  0.34755510464310646\n",
      "Epoch  16 loss:  0.3409371804445982\n",
      "Epoch  17 loss:  0.33449243940413\n",
      "Epoch  18 loss:  0.31812382116913795\n",
      "Epoch  19 loss:  0.2936648391187191\n",
      "Epoch  20 loss:  0.2809950262308121\n",
      "Epoch  21 loss:  0.26867179945111275\n",
      "Epoch  22 loss:  0.2552047111093998\n",
      "Epoch  23 loss:  0.24700164794921875\n",
      "Epoch  24 loss:  0.2412311714142561\n",
      "Epoch  25 loss:  0.23697298392653465\n",
      "Epoch  26 loss:  0.23361879028379917\n",
      "Epoch  27 loss:  0.23076199274510145\n",
      "Epoch  28 loss:  0.2282998040318489\n",
      "Epoch  29 loss:  0.22614654432982206\n",
      "Epoch  30 loss:  0.22418398689478636\n",
      "Epoch  31 loss:  0.2224441608414054\n",
      "Epoch  32 loss:  0.22079989034682512\n",
      "Epoch  33 loss:  0.21920483279973269\n",
      "Epoch  34 loss:  0.21742004714906216\n",
      "Epoch  35 loss:  0.21563201118260622\n",
      "Epoch  36 loss:  0.2139668557792902\n",
      "Epoch  37 loss:  0.21240944415330887\n",
      "Epoch  38 loss:  0.21082057058811188\n",
      "Epoch  39 loss:  0.20929901860654354\n",
      "Epoch  40 loss:  0.20797820389270782\n",
      "Epoch  41 loss:  0.20690983440726995\n",
      "Epoch  42 loss:  0.2059553526341915\n",
      "Epoch  43 loss:  0.20508569944649935\n",
      "Epoch  44 loss:  0.2042466076090932\n",
      "Epoch  45 loss:  0.20343125332146883\n",
      "Epoch  46 loss:  0.20266911201179028\n",
      "Epoch  47 loss:  0.20198484882712364\n",
      "Epoch  48 loss:  0.20135967899113894\n",
      "Epoch  49 loss:  0.20076916832476854\n",
      "Epoch  50 loss:  0.20018500369042158\n",
      "Epoch  51 loss:  0.19955139700323343\n",
      "Epoch  52 loss:  0.19895689003169537\n",
      "Epoch  53 loss:  0.19838352408260107\n",
      "Epoch  54 loss:  0.19786198064684868\n",
      "Epoch  55 loss:  0.19736605882644653\n",
      "Epoch  56 loss:  0.19689185731112957\n",
      "Epoch  57 loss:  0.19644315261393785\n",
      "Epoch  58 loss:  0.19599760323762894\n",
      "Epoch  59 loss:  0.19554596673697233\n",
      "Epoch  60 loss:  0.19507179036736488\n",
      "Epoch  61 loss:  0.1946608852595091\n",
      "Epoch  62 loss:  0.1942895445972681\n",
      "Epoch  63 loss:  0.1939044976606965\n",
      "Epoch  64 loss:  0.19353767856955528\n",
      "Epoch  65 loss:  0.1931757414713502\n",
      "Epoch  66 loss:  0.1928344750776887\n",
      "Epoch  67 loss:  0.19252193346619606\n",
      "Epoch  68 loss:  0.1922092866152525\n",
      "Epoch  69 loss:  0.19190066494047642\n",
      "Epoch  70 loss:  0.1916293390095234\n",
      "Epoch  71 loss:  0.19138384331017733\n",
      "Epoch  72 loss:  0.19113864097744226\n",
      "Epoch  73 loss:  0.19091511331498623\n",
      "Epoch  74 loss:  0.1907054129987955\n",
      "Epoch  75 loss:  0.1905388655140996\n",
      "Epoch  76 loss:  0.19041163381189108\n",
      "Epoch  77 loss:  0.1903123687952757\n",
      "Epoch  78 loss:  0.1902283886447549\n",
      "Epoch  79 loss:  0.19010377954691648\n",
      "Epoch  80 loss:  0.18993916362524033\n",
      "Epoch  81 loss:  0.18975393194705248\n",
      "Epoch  82 loss:  0.189645417034626\n",
      "Epoch  83 loss:  0.18964311107993126\n",
      "Epoch  84 loss:  0.18964868690818548\n",
      "Epoch  85 loss:  0.18954289238899946\n",
      "Epoch  86 loss:  0.18946882337331772\n",
      "Epoch  87 loss:  0.18949760682880878\n",
      "Epoch  88 loss:  0.1894874321296811\n",
      "Epoch  89 loss:  0.1892488244920969\n",
      "Epoch  90 loss:  0.18909563682973385\n",
      "Epoch  91 loss:  0.1890674065798521\n",
      "Epoch  92 loss:  0.18900386057794094\n",
      "Epoch  93 loss:  0.18870687671005726\n",
      "Epoch  94 loss:  0.18852559197694063\n",
      "Epoch  95 loss:  0.18820305820554495\n",
      "Epoch  96 loss:  0.18782840203493834\n",
      "Epoch  97 loss:  0.18752496317029\n",
      "Epoch  98 loss:  0.1871988521888852\n",
      "Epoch  99 loss:  0.18702793959528208\n",
      "Epoch  100 loss:  0.18692502565681934\n",
      "Epoch  101 loss:  0.18655575159937143\n",
      "Epoch  102 loss:  0.18609501142054796\n",
      "Epoch  103 loss:  0.18577786907553673\n",
      "Epoch  104 loss:  0.18550832476466894\n",
      "Epoch  105 loss:  0.18532292172312737\n",
      "Epoch  106 loss:  0.18526498693972826\n",
      "Epoch  107 loss:  0.18517118506133556\n",
      "Epoch  108 loss:  0.184977768920362\n",
      "Epoch  109 loss:  0.18472877144813538\n",
      "Epoch  110 loss:  0.18447005935013294\n",
      "Epoch  111 loss:  0.18430221173912287\n",
      "Epoch  112 loss:  0.18414574768394232\n",
      "Epoch  113 loss:  0.18398413620889187\n",
      "Epoch  114 loss:  0.18386711739003658\n",
      "Epoch  115 loss:  0.18393416050821543\n",
      "Epoch  116 loss:  0.18395926617085934\n",
      "Epoch  117 loss:  0.18389052897691727\n",
      "Epoch  118 loss:  0.18374151829630136\n",
      "Epoch  119 loss:  0.1836011204868555\n",
      "Epoch  120 loss:  0.18369382433593273\n",
      "Epoch  121 loss:  0.18386848736554384\n",
      "Epoch  122 loss:  0.18377303425222635\n",
      "Epoch  123 loss:  0.1836219523102045\n",
      "Epoch  124 loss:  0.18347514513880014\n",
      "Epoch  125 loss:  0.1834433600306511\n",
      "Epoch  126 loss:  0.18331705313175917\n",
      "Epoch  127 loss:  0.18307921569794416\n",
      "Epoch  128 loss:  0.18244376126676798\n",
      "Epoch  129 loss:  0.1815650276839733\n",
      "Epoch  130 loss:  0.18096950557082891\n",
      "Epoch  131 loss:  0.18052573408931494\n",
      "Epoch  132 loss:  0.18022979609668255\n",
      "Epoch  133 loss:  0.17977273650467396\n",
      "Epoch  134 loss:  0.17944681365042925\n",
      "Epoch  135 loss:  0.17939014174044132\n",
      "Epoch  136 loss:  0.1793595887720585\n",
      "Epoch  137 loss:  0.17910397611558437\n",
      "Epoch  138 loss:  0.17899451032280922\n",
      "Epoch  139 loss:  0.1789137525483966\n",
      "Epoch  140 loss:  0.17890294827520847\n",
      "Epoch  141 loss:  0.17867137398570776\n",
      "Epoch  142 loss:  0.17831357568502426\n",
      "Epoch  143 loss:  0.17810893151909113\n",
      "Epoch  144 loss:  0.17789005301892757\n",
      "Epoch  145 loss:  0.17731774784624577\n",
      "Epoch  146 loss:  0.1767075713723898\n",
      "Epoch  147 loss:  0.17583614494651556\n",
      "Epoch  148 loss:  0.1754085998982191\n",
      "Epoch  149 loss:  0.17514410242438316\n",
      "Epoch  150 loss:  0.1748283188790083\n",
      "Epoch  151 loss:  0.17449971474707127\n",
      "Epoch  152 loss:  0.17432493064552546\n",
      "Epoch  153 loss:  0.1741331797093153\n",
      "Epoch  154 loss:  0.1739172302186489\n",
      "Epoch  155 loss:  0.17371816094964743\n",
      "Epoch  156 loss:  0.17371339723467827\n",
      "Epoch  157 loss:  0.1736491173505783\n",
      "Epoch  158 loss:  0.17339426558464766\n",
      "Epoch  159 loss:  0.17335014417767525\n",
      "Epoch  160 loss:  0.1733175441622734\n",
      "Epoch  161 loss:  0.17313676048070192\n",
      "Epoch  162 loss:  0.17299640644341707\n",
      "Epoch  163 loss:  0.17299765162169933\n",
      "Epoch  164 loss:  0.17295200377702713\n",
      "Epoch  165 loss:  0.172840753570199\n",
      "Epoch  166 loss:  0.17281597293913364\n",
      "Epoch  167 loss:  0.1728316806256771\n",
      "Epoch  168 loss:  0.17269833199679852\n",
      "Epoch  169 loss:  0.1726130722090602\n",
      "Epoch  170 loss:  0.17277929186820984\n",
      "Epoch  171 loss:  0.1728522265329957\n",
      "Epoch  172 loss:  0.17261604312807322\n",
      "Epoch  173 loss:  0.17256537359207869\n",
      "Epoch  174 loss:  0.17244994547218084\n",
      "Epoch  175 loss:  0.1722704851999879\n",
      "Epoch  176 loss:  0.17221418861299753\n",
      "Epoch  177 loss:  0.17223971523344517\n",
      "Epoch  178 loss:  0.1720738410949707\n",
      "Epoch  179 loss:  0.1720434669405222\n",
      "Epoch  180 loss:  0.1720426855608821\n",
      "Epoch  181 loss:  0.17196939419955015\n",
      "Epoch  182 loss:  0.17190120369195938\n",
      "Epoch  183 loss:  0.17193837836384773\n",
      "Epoch  184 loss:  0.171959244646132\n",
      "Epoch  185 loss:  0.17207441199570894\n",
      "Epoch  186 loss:  0.17220596130937338\n",
      "Epoch  187 loss:  0.17230993043631315\n",
      "Epoch  188 loss:  0.17227134760469198\n",
      "Epoch  189 loss:  0.17226060014218092\n",
      "Epoch  190 loss:  0.17239628080278635\n",
      "Epoch  191 loss:  0.17251835390925407\n",
      "Epoch  192 loss:  0.17238827142864466\n",
      "Epoch  193 loss:  0.1722448468208313\n",
      "Epoch  194 loss:  0.1722179278731346\n",
      "Epoch  195 loss:  0.17237335816025734\n",
      "Epoch  196 loss:  0.172376056201756\n",
      "Epoch  197 loss:  0.17227299232035875\n",
      "Epoch  198 loss:  0.1723409416154027\n",
      "Epoch  199 loss:  0.17254725098609924\n",
      "Epoch  200 loss:  0.17253718245774508\n",
      "Epoch  201 loss:  0.17233805358409882\n",
      "Epoch  202 loss:  0.172374258749187\n",
      "Epoch  203 loss:  0.17252015136182308\n",
      "Epoch  204 loss:  0.1725524701178074\n",
      "Epoch  205 loss:  0.17240290436893702\n",
      "Epoch  206 loss:  0.17235902231186628\n",
      "Epoch  207 loss:  0.17210473492741585\n",
      "Epoch  208 loss:  0.1713191643357277\n",
      "Epoch  209 loss:  0.1704771090298891\n",
      "Epoch  210 loss:  0.16939997114241123\n",
      "Epoch  211 loss:  0.16866338811814785\n",
      "Epoch  212 loss:  0.16826475784182549\n",
      "Epoch  213 loss:  0.16777458507567644\n",
      "Epoch  214 loss:  0.16744943615049124\n",
      "Epoch  215 loss:  0.16732995957136154\n",
      "Epoch  216 loss:  0.16718142200261354\n",
      "Epoch  217 loss:  0.16716330498456955\n",
      "Epoch  218 loss:  0.16705330088734627\n",
      "Epoch  219 loss:  0.16675269603729248\n",
      "Epoch  220 loss:  0.16633730847388506\n",
      "Epoch  221 loss:  0.16610122751444578\n",
      "Epoch  222 loss:  0.16584886983036995\n",
      "Epoch  223 loss:  0.16565530095249414\n",
      "Epoch  224 loss:  0.1655981605872512\n",
      "Epoch  225 loss:  0.16564880404621363\n",
      "Epoch  226 loss:  0.1656340267509222\n",
      "Epoch  227 loss:  0.1655919337645173\n",
      "Epoch  228 loss:  0.16554983239620924\n",
      "Epoch  229 loss:  0.16548775136470795\n",
      "Epoch  230 loss:  0.1652371035888791\n",
      "Epoch  231 loss:  0.16507168021053076\n",
      "Epoch  232 loss:  0.16502321232110262\n",
      "Epoch  233 loss:  0.1650131195783615\n",
      "Epoch  234 loss:  0.16493426356464624\n",
      "Epoch  235 loss:  0.164758138358593\n",
      "Epoch  236 loss:  0.16478281188756227\n",
      "Epoch  237 loss:  0.16478390153497458\n",
      "Epoch  238 loss:  0.1647338792681694\n",
      "Epoch  239 loss:  0.16467489395290613\n",
      "Epoch  240 loss:  0.16463016904890537\n",
      "Epoch  241 loss:  0.16471524257212877\n",
      "Epoch  242 loss:  0.1647239699959755\n",
      "Epoch  243 loss:  0.16477425303310156\n",
      "Epoch  244 loss:  0.16493331920355558\n",
      "Epoch  245 loss:  0.16496352292597294\n",
      "Epoch  246 loss:  0.16496192291378975\n",
      "Epoch  247 loss:  0.16513578593730927\n",
      "Epoch  248 loss:  0.16511844005435705\n",
      "Epoch  249 loss:  0.16499122511595488\n",
      "Epoch  250 loss:  0.16497662104666233\n",
      "Epoch  251 loss:  0.1650306023657322\n",
      "Epoch  252 loss:  0.1649457858875394\n",
      "Epoch  253 loss:  0.1647658422589302\n",
      "Epoch  254 loss:  0.16462139319628477\n",
      "Epoch  255 loss:  0.16455447860062122\n",
      "Epoch  256 loss:  0.1644080923870206\n",
      "Epoch  257 loss:  0.16444178763777018\n",
      "Epoch  258 loss:  0.16439749859273434\n",
      "Epoch  259 loss:  0.16417496744543314\n",
      "Epoch  260 loss:  0.16418607160449028\n",
      "Epoch  261 loss:  0.16427928302437067\n",
      "Epoch  262 loss:  0.16427044291049242\n",
      "Epoch  263 loss:  0.16391621064394712\n",
      "Epoch  264 loss:  0.16364974901080132\n",
      "Epoch  265 loss:  0.1634738054126501\n",
      "Epoch  266 loss:  0.16340751759707928\n",
      "Epoch  267 loss:  0.16342451144009829\n",
      "Epoch  268 loss:  0.1633064765483141\n",
      "Epoch  269 loss:  0.1632707016542554\n",
      "Epoch  270 loss:  0.16338918823748827\n",
      "Epoch  271 loss:  0.163220445625484\n",
      "Epoch  272 loss:  0.16303708497434855\n",
      "Epoch  273 loss:  0.16284990217536688\n",
      "Epoch  274 loss:  0.16276341956108809\n",
      "Epoch  275 loss:  0.16286190133541822\n",
      "Epoch  276 loss:  0.16296192072331905\n",
      "Epoch  277 loss:  0.16304574348032475\n",
      "Epoch  278 loss:  0.1630733199417591\n",
      "Epoch  279 loss:  0.16300091054290533\n",
      "Epoch  280 loss:  0.16296885069459677\n",
      "Epoch  281 loss:  0.16295340191572905\n",
      "Epoch  282 loss:  0.16278590634465218\n",
      "Epoch  283 loss:  0.16269404627382755\n",
      "Epoch  284 loss:  0.16272537037730217\n",
      "Epoch  285 loss:  0.16262562479823828\n",
      "Epoch  286 loss:  0.16232719086110592\n",
      "Epoch  287 loss:  0.16218352876603603\n",
      "Epoch  288 loss:  0.16219055652618408\n",
      "Epoch  289 loss:  0.1619363073259592\n",
      "Epoch  290 loss:  0.16184961702674627\n",
      "Epoch  291 loss:  0.16195644531399012\n",
      "Epoch  292 loss:  0.16177603602409363\n",
      "Epoch  293 loss:  0.16170789301395416\n",
      "Epoch  294 loss:  0.16173943039029837\n",
      "Epoch  295 loss:  0.16171271912753582\n",
      "Epoch  296 loss:  0.16146762017160654\n",
      "Epoch  297 loss:  0.16151414159685373\n",
      "Epoch  298 loss:  0.16153063159435987\n",
      "Epoch  299 loss:  0.16125427465885878\n",
      "Epoch  300 loss:  0.161080127581954\n",
      "Epoch  301 loss:  0.16097116563469172\n",
      "Epoch  302 loss:  0.1608866984024644\n",
      "Epoch  303 loss:  0.16083448380231857\n",
      "Epoch  304 loss:  0.16079390048980713\n",
      "Epoch  305 loss:  0.16076910309493542\n",
      "Epoch  306 loss:  0.16073819156736135\n",
      "Epoch  307 loss:  0.1607462354004383\n",
      "Epoch  308 loss:  0.16066733747720718\n",
      "Epoch  309 loss:  0.16073501482605934\n",
      "Epoch  310 loss:  0.16082980670034885\n",
      "Epoch  311 loss:  0.16078476514667273\n",
      "Epoch  312 loss:  0.1606405284255743\n",
      "Epoch  313 loss:  0.16055622976273298\n",
      "Epoch  314 loss:  0.1604031464084983\n",
      "Epoch  315 loss:  0.16030371841043234\n",
      "Epoch  316 loss:  0.16015955340117216\n",
      "Epoch  317 loss:  0.16013481561094522\n",
      "Epoch  318 loss:  0.1600711438804865\n",
      "Epoch  319 loss:  0.15995227172970772\n",
      "Epoch  320 loss:  0.159934738650918\n",
      "Epoch  321 loss:  0.1599173303693533\n",
      "Epoch  322 loss:  0.15983081329613924\n",
      "Epoch  323 loss:  0.15976498927921057\n",
      "Epoch  324 loss:  0.1598038962110877\n",
      "Epoch  325 loss:  0.15985332801938057\n",
      "Epoch  326 loss:  0.159941959194839\n",
      "Epoch  327 loss:  0.15987466741353273\n",
      "Epoch  328 loss:  0.1598202856257558\n",
      "Epoch  329 loss:  0.15993991121649742\n",
      "Epoch  330 loss:  0.15998521354049444\n",
      "Epoch  331 loss:  0.15985224209725857\n",
      "Epoch  332 loss:  0.15977972839027643\n",
      "Epoch  333 loss:  0.15976019017398357\n",
      "Epoch  334 loss:  0.15985678788274527\n",
      "Epoch  335 loss:  0.15986021514981985\n",
      "Epoch  336 loss:  0.1596356015652418\n",
      "Epoch  337 loss:  0.15950216073542833\n",
      "Epoch  338 loss:  0.15963712893426418\n",
      "Epoch  339 loss:  0.1596027836203575\n",
      "Epoch  340 loss:  0.1594386762008071\n",
      "Epoch  341 loss:  0.15958288870751858\n",
      "Epoch  342 loss:  0.1596796764060855\n",
      "Epoch  343 loss:  0.1595891211181879\n",
      "Epoch  344 loss:  0.15942535921931267\n",
      "Epoch  345 loss:  0.1593696791678667\n",
      "Epoch  346 loss:  0.1594511717557907\n",
      "Epoch  347 loss:  0.15944665018469095\n",
      "Epoch  348 loss:  0.15941290371119976\n",
      "Epoch  349 loss:  0.1595593085512519\n",
      "Epoch  350 loss:  0.15964714158326387\n",
      "Epoch  351 loss:  0.15951374918222427\n",
      "Epoch  352 loss:  0.15946793369948864\n",
      "Epoch  353 loss:  0.1596540343016386\n",
      "Epoch  354 loss:  0.1595039926469326\n",
      "Epoch  355 loss:  0.1594345150515437\n",
      "Epoch  356 loss:  0.15944120101630688\n",
      "Epoch  357 loss:  0.15930451732128859\n",
      "Epoch  358 loss:  0.1591254062950611\n",
      "Epoch  359 loss:  0.15921199414879084\n",
      "Epoch  360 loss:  0.15923001244664192\n",
      "Epoch  361 loss:  0.15900874510407448\n",
      "Epoch  362 loss:  0.15905970986932516\n",
      "Epoch  363 loss:  0.15916003938764334\n",
      "Epoch  364 loss:  0.15910927671939135\n",
      "Epoch  365 loss:  0.15915564727038145\n",
      "Epoch  366 loss:  0.15930512361228466\n",
      "Epoch  367 loss:  0.15920850541442633\n",
      "Epoch  368 loss:  0.15917544160038233\n",
      "Epoch  369 loss:  0.15919329226016998\n",
      "Epoch  370 loss:  0.15910144336521626\n",
      "Epoch  371 loss:  0.1590795675292611\n",
      "Epoch  372 loss:  0.15920384787023067\n",
      "Epoch  373 loss:  0.15928685013204813\n",
      "Epoch  374 loss:  0.15940971300005913\n",
      "Epoch  375 loss:  0.1595137370750308\n",
      "Epoch  376 loss:  0.15967593621462584\n",
      "Epoch  377 loss:  0.15982072707265615\n",
      "Epoch  378 loss:  0.16009279899299145\n",
      "Epoch  379 loss:  0.16061553359031677\n",
      "Epoch  380 loss:  0.16076988354325294\n",
      "Epoch  381 loss:  0.16082426998764277\n",
      "Epoch  382 loss:  0.16104236152023077\n",
      "Epoch  383 loss:  0.16137161385267973\n",
      "Epoch  384 loss:  0.16122306510806084\n",
      "Epoch  385 loss:  0.16103549022227526\n",
      "Epoch  386 loss:  0.16113227047026157\n",
      "Epoch  387 loss:  0.16133237443864346\n",
      "Epoch  388 loss:  0.16098321601748466\n",
      "Epoch  389 loss:  0.16043256968259811\n",
      "Epoch  390 loss:  0.16033461689949036\n",
      "Epoch  391 loss:  0.16009975597262383\n",
      "Epoch  392 loss:  0.15977938007563353\n",
      "Epoch  393 loss:  0.15934877190738916\n",
      "Epoch  394 loss:  0.15907033160328865\n",
      "Epoch  395 loss:  0.15895510651171207\n",
      "Epoch  396 loss:  0.15878148283809423\n",
      "Epoch  397 loss:  0.1585910553112626\n",
      "Epoch  398 loss:  0.15817038714885712\n",
      "Epoch  399 loss:  0.15752278361469507\n",
      "Epoch  400 loss:  0.1568618817254901\n",
      "Epoch  401 loss:  0.15629821177572012\n",
      "Epoch  402 loss:  0.1560113374143839\n",
      "Epoch  403 loss:  0.1556581249460578\n",
      "Epoch  404 loss:  0.15545619372278452\n",
      "Epoch  405 loss:  0.15519968327134848\n",
      "Epoch  406 loss:  0.15492882672697306\n",
      "Epoch  407 loss:  0.1548041719943285\n",
      "Epoch  408 loss:  0.15475800447165966\n",
      "Epoch  409 loss:  0.15467298682779074\n",
      "Epoch  410 loss:  0.15469528455287218\n",
      "Epoch  411 loss:  0.15469944663345814\n",
      "Epoch  412 loss:  0.15466156788170338\n",
      "Epoch  413 loss:  0.15468619298189878\n",
      "Epoch  414 loss:  0.15454229712486267\n",
      "Epoch  415 loss:  0.1543497247621417\n",
      "Epoch  416 loss:  0.15433829929679632\n",
      "Epoch  417 loss:  0.15428603626787663\n",
      "Epoch  418 loss:  0.15409509930759668\n",
      "Epoch  419 loss:  0.15398088656365871\n",
      "Epoch  420 loss:  0.1540118344128132\n",
      "Epoch  421 loss:  0.15411662310361862\n",
      "Epoch  422 loss:  0.15409425552934408\n",
      "Epoch  423 loss:  0.1540408469736576\n",
      "Epoch  424 loss:  0.1539530074223876\n",
      "Epoch  425 loss:  0.1539131784811616\n",
      "Epoch  426 loss:  0.153845920227468\n",
      "Epoch  427 loss:  0.15363950468599796\n",
      "Epoch  428 loss:  0.15345888398587704\n",
      "Epoch  429 loss:  0.15348913054913282\n",
      "Epoch  430 loss:  0.15359537210315466\n",
      "Epoch  431 loss:  0.15361758414655924\n",
      "Epoch  432 loss:  0.15354920737445354\n",
      "Epoch  433 loss:  0.15359602589160204\n",
      "Epoch  434 loss:  0.15362929552793503\n",
      "Epoch  435 loss:  0.15354649163782597\n",
      "Epoch  436 loss:  0.15352970734238625\n",
      "Epoch  437 loss:  0.153537193313241\n",
      "Epoch  438 loss:  0.1534882066771388\n",
      "Epoch  439 loss:  0.15339093189686537\n",
      "Epoch  440 loss:  0.15339660178869963\n",
      "Epoch  441 loss:  0.15334920771420002\n",
      "Epoch  442 loss:  0.15327155590057373\n",
      "Epoch  443 loss:  0.15324078127741814\n",
      "Epoch  444 loss:  0.15332909673452377\n",
      "Epoch  445 loss:  0.1532415570691228\n",
      "Epoch  446 loss:  0.1531434915959835\n",
      "Epoch  447 loss:  0.15316462516784668\n",
      "Epoch  448 loss:  0.1531653068959713\n",
      "Epoch  449 loss:  0.153089408762753\n",
      "Epoch  450 loss:  0.1530144391581416\n",
      "Epoch  451 loss:  0.15301332715898752\n",
      "Epoch  452 loss:  0.15295601729303598\n",
      "Epoch  453 loss:  0.15276731736958027\n",
      "Epoch  454 loss:  0.1527300924062729\n",
      "Epoch  455 loss:  0.15268138702958822\n",
      "Epoch  456 loss:  0.15249508526176214\n",
      "Epoch  457 loss:  0.15236138831824064\n",
      "Epoch  458 loss:  0.15239816345274448\n",
      "Epoch  459 loss:  0.15241297520697117\n",
      "Epoch  460 loss:  0.15238456055521965\n",
      "Epoch  461 loss:  0.15231582429260015\n",
      "Epoch  462 loss:  0.1522321216762066\n",
      "Epoch  463 loss:  0.1521459138020873\n",
      "Epoch  464 loss:  0.1520625576376915\n",
      "Epoch  465 loss:  0.15204523038119078\n",
      "Epoch  466 loss:  0.15213296376168728\n",
      "Epoch  467 loss:  0.15207601711153984\n",
      "Epoch  468 loss:  0.15200004447251558\n",
      "Epoch  469 loss:  0.15200974512845278\n",
      "Epoch  470 loss:  0.1519654504954815\n",
      "Epoch  471 loss:  0.15181573759764433\n",
      "Epoch  472 loss:  0.1518080085515976\n",
      "Epoch  473 loss:  0.15185991488397121\n",
      "Epoch  474 loss:  0.1520556788891554\n",
      "Epoch  475 loss:  0.15208769217133522\n",
      "Epoch  476 loss:  0.15226249396800995\n",
      "Epoch  477 loss:  0.15233866404742002\n",
      "Epoch  478 loss:  0.15247522573918104\n",
      "Epoch  479 loss:  0.15263161435723305\n",
      "Epoch  480 loss:  0.15292076487094164\n",
      "Epoch  481 loss:  0.15300318598747253\n",
      "Epoch  482 loss:  0.15306659042835236\n",
      "Epoch  483 loss:  0.15322078578174114\n",
      "Epoch  484 loss:  0.1534334234893322\n",
      "Epoch  485 loss:  0.15380545146763325\n",
      "Epoch  486 loss:  0.15413905307650566\n",
      "Epoch  487 loss:  0.15440599899739027\n",
      "Epoch  488 loss:  0.15459825564175844\n",
      "Epoch  489 loss:  0.154596958309412\n",
      "Epoch  490 loss:  0.15490373875945807\n",
      "Epoch  491 loss:  0.15470234956592321\n",
      "Epoch  492 loss:  0.15421226061880589\n",
      "Epoch  493 loss:  0.15395405050367117\n",
      "Epoch  494 loss:  0.15375080332159996\n",
      "Epoch  495 loss:  0.1536792814731598\n",
      "Epoch  496 loss:  0.15315079409629107\n",
      "Epoch  497 loss:  0.15267092734575272\n",
      "Epoch  498 loss:  0.15245009306818247\n",
      "Epoch  499 loss:  0.15206681564450264\n",
      "Epoch  500 loss:  0.15173717867583036\n",
      "Epoch  501 loss:  0.15138848405331373\n",
      "Epoch  502 loss:  0.1512552136555314\n",
      "Epoch  503 loss:  0.15099672507494688\n",
      "Epoch  504 loss:  0.15072344988584518\n",
      "Epoch  505 loss:  0.15046643558889627\n",
      "Epoch  506 loss:  0.15024625323712826\n",
      "Epoch  507 loss:  0.15006256010383368\n",
      "Epoch  508 loss:  0.149929310195148\n",
      "Epoch  509 loss:  0.14993438683450222\n",
      "Epoch  510 loss:  0.15001599583774805\n",
      "Epoch  511 loss:  0.1501191407442093\n",
      "Epoch  512 loss:  0.1502482397481799\n",
      "Epoch  513 loss:  0.15025791618973017\n",
      "Epoch  514 loss:  0.15039677172899246\n",
      "Epoch  515 loss:  0.15059381257742643\n",
      "Epoch  516 loss:  0.1505265301093459\n",
      "Epoch  517 loss:  0.150279700756073\n",
      "Epoch  518 loss:  0.15013105142861605\n",
      "Epoch  519 loss:  0.15018939692527056\n",
      "Epoch  520 loss:  0.15033407788723707\n",
      "Epoch  521 loss:  0.15024046879261732\n",
      "Epoch  522 loss:  0.15019159484654665\n",
      "Epoch  523 loss:  0.15017694793641567\n",
      "Epoch  524 loss:  0.14999878872185946\n",
      "Epoch  525 loss:  0.1498803896829486\n",
      "Epoch  526 loss:  0.14999245572835207\n",
      "Epoch  527 loss:  0.15003022737801075\n",
      "Epoch  528 loss:  0.14994492009282112\n",
      "Epoch  529 loss:  0.14994015730917454\n",
      "Epoch  530 loss:  0.14997958578169346\n",
      "Epoch  531 loss:  0.14994860254228115\n",
      "Epoch  532 loss:  0.1496240133419633\n",
      "Epoch  533 loss:  0.149403122253716\n",
      "Epoch  534 loss:  0.14940411504358053\n",
      "Epoch  535 loss:  0.14925773441791534\n",
      "Epoch  536 loss:  0.14908232260495424\n",
      "Epoch  537 loss:  0.149028024636209\n",
      "Epoch  538 loss:  0.14895965810865164\n",
      "Epoch  539 loss:  0.14884503837674856\n",
      "Epoch  540 loss:  0.14886334724724293\n",
      "Epoch  541 loss:  0.148898814804852\n",
      "Epoch  542 loss:  0.14881650637835264\n",
      "Epoch  543 loss:  0.14884732477366924\n",
      "Epoch  544 loss:  0.14892101474106312\n",
      "Epoch  545 loss:  0.14888168964534998\n",
      "Epoch  546 loss:  0.1488176267594099\n",
      "Epoch  547 loss:  0.14880726486444473\n",
      "Epoch  548 loss:  0.14886195212602615\n",
      "Epoch  549 loss:  0.14878920651972294\n",
      "Epoch  550 loss:  0.14874374773353338\n",
      "Epoch  551 loss:  0.148683144710958\n",
      "Epoch  552 loss:  0.14872446656227112\n",
      "Epoch  553 loss:  0.1488270265981555\n",
      "Epoch  554 loss:  0.1488899625837803\n",
      "Epoch  555 loss:  0.1488375086337328\n",
      "Epoch  556 loss:  0.14872475992888212\n",
      "Epoch  557 loss:  0.14868426229804754\n",
      "Epoch  558 loss:  0.1488208705559373\n",
      "Epoch  559 loss:  0.14886003639549017\n",
      "Epoch  560 loss:  0.14880246482789516\n",
      "Epoch  561 loss:  0.14885732997208834\n",
      "Epoch  562 loss:  0.14906348939985037\n",
      "Epoch  563 loss:  0.1490558786317706\n",
      "Epoch  564 loss:  0.14900962356477976\n",
      "Epoch  565 loss:  0.14914454333484173\n",
      "Epoch  566 loss:  0.1491042785346508\n",
      "Epoch  567 loss:  0.14905772171914577\n",
      "Epoch  568 loss:  0.14904893469065428\n",
      "Epoch  569 loss:  0.1490014148876071\n",
      "Epoch  570 loss:  0.14880873076617718\n",
      "Epoch  571 loss:  0.14882443007081747\n",
      "Epoch  572 loss:  0.14873926900327206\n",
      "Epoch  573 loss:  0.1484653502702713\n",
      "Epoch  574 loss:  0.14844633266329765\n",
      "Epoch  575 loss:  0.1486446624621749\n",
      "Epoch  576 loss:  0.14853446371853352\n",
      "Epoch  577 loss:  0.14843393675982952\n",
      "Epoch  578 loss:  0.14849012810736895\n",
      "Epoch  579 loss:  0.14833034668117762\n",
      "Epoch  580 loss:  0.14819022826850414\n",
      "Epoch  581 loss:  0.1482220208272338\n",
      "Epoch  582 loss:  0.14820622373372316\n",
      "Epoch  583 loss:  0.14823096990585327\n",
      "Epoch  584 loss:  0.14827405102550983\n",
      "Epoch  585 loss:  0.1481195604428649\n",
      "Epoch  586 loss:  0.1480691870674491\n",
      "Epoch  587 loss:  0.14820931386202574\n",
      "Epoch  588 loss:  0.14831487089395523\n",
      "Epoch  589 loss:  0.14830041211098433\n",
      "Epoch  590 loss:  0.14831487089395523\n",
      "Epoch  591 loss:  0.1482949936762452\n",
      "Epoch  592 loss:  0.14814433082938194\n",
      "Epoch  593 loss:  0.14815753418952227\n",
      "Epoch  594 loss:  0.1482244161888957\n",
      "Epoch  595 loss:  0.14824689459055662\n",
      "Epoch  596 loss:  0.14825273491442204\n",
      "Epoch  597 loss:  0.14824701473116875\n",
      "Epoch  598 loss:  0.1483085723593831\n",
      "Epoch  599 loss:  0.1483221985399723\n",
      "Epoch  600 loss:  0.14829786960035563\n",
      "Epoch  601 loss:  0.1483378428965807\n",
      "Epoch  602 loss:  0.14839834813028574\n",
      "Epoch  603 loss:  0.14841869287192822\n",
      "Epoch  604 loss:  0.14813627023249865\n",
      "Epoch  605 loss:  0.14796239603310823\n",
      "Epoch  606 loss:  0.14789943117648363\n",
      "Epoch  607 loss:  0.1476984666660428\n",
      "Epoch  608 loss:  0.14761140663176775\n",
      "Epoch  609 loss:  0.14755850844085217\n",
      "Epoch  610 loss:  0.1470553269609809\n",
      "Epoch  611 loss:  0.14638108760118484\n",
      "Epoch  612 loss:  0.14574224036186934\n",
      "Epoch  613 loss:  0.14361556619405746\n",
      "Epoch  614 loss:  0.1422899472527206\n",
      "Epoch  615 loss:  0.14176306128501892\n",
      "Epoch  616 loss:  0.14131550770252943\n",
      "Epoch  617 loss:  0.14092734456062317\n",
      "Epoch  618 loss:  0.1406506933271885\n",
      "Epoch  619 loss:  0.14020887156948447\n",
      "Epoch  620 loss:  0.13997728610411286\n",
      "Epoch  621 loss:  0.13972243759781122\n",
      "Epoch  622 loss:  0.13964892970398068\n",
      "Epoch  623 loss:  0.13922827318310738\n",
      "Epoch  624 loss:  0.13884431403130293\n",
      "Epoch  625 loss:  0.13847564300522208\n",
      "Epoch  626 loss:  0.13794489530846477\n",
      "Epoch  627 loss:  0.13765564793720841\n",
      "Epoch  628 loss:  0.1374115152284503\n",
      "Epoch  629 loss:  0.13725454732775688\n",
      "Epoch  630 loss:  0.13704391615465283\n",
      "Epoch  631 loss:  0.13701266748830676\n",
      "Epoch  632 loss:  0.13701565330848098\n",
      "Epoch  633 loss:  0.1369350808672607\n",
      "Epoch  634 loss:  0.13688520342111588\n",
      "Epoch  635 loss:  0.13686813786625862\n",
      "Epoch  636 loss:  0.13686448195949197\n",
      "Epoch  637 loss:  0.13675571139901876\n",
      "Epoch  638 loss:  0.13693619845435023\n",
      "Epoch  639 loss:  0.13689917977899313\n",
      "Epoch  640 loss:  0.13694725604727864\n",
      "Epoch  641 loss:  0.1370758437551558\n",
      "Epoch  642 loss:  0.13723716652020812\n",
      "Epoch  643 loss:  0.13757631927728653\n",
      "Epoch  644 loss:  0.1382288127206266\n",
      "Epoch  645 loss:  0.1387925511226058\n",
      "Epoch  646 loss:  0.1392041533254087\n",
      "Epoch  647 loss:  0.13963005505502224\n",
      "Epoch  648 loss:  0.1397065557539463\n",
      "Epoch  649 loss:  0.13987864088267088\n",
      "Epoch  650 loss:  0.13975232653319836\n",
      "Epoch  651 loss:  0.1393699711188674\n",
      "Epoch  652 loss:  0.13885116390883923\n",
      "Epoch  653 loss:  0.13816244946792722\n",
      "Epoch  654 loss:  0.1381674325093627\n",
      "Epoch  655 loss:  0.13813146436586976\n",
      "Epoch  656 loss:  0.13753761257976294\n",
      "Epoch  657 loss:  0.13696315279230475\n",
      "Epoch  658 loss:  0.13640542142093182\n",
      "Epoch  659 loss:  0.13597047375515103\n",
      "Epoch  660 loss:  0.13569228630512953\n",
      "Epoch  661 loss:  0.1357794194482267\n",
      "Epoch  662 loss:  0.13620211044326425\n",
      "Epoch  663 loss:  0.13661734154447913\n",
      "Epoch  664 loss:  0.13688101852312684\n",
      "Epoch  665 loss:  0.136793015524745\n",
      "Epoch  666 loss:  0.13687723223119974\n",
      "Epoch  667 loss:  0.13688987959176302\n",
      "Epoch  668 loss:  0.13624959997832775\n",
      "Epoch  669 loss:  0.13570521119982004\n",
      "Epoch  670 loss:  0.1354981712065637\n",
      "Epoch  671 loss:  0.13469172595068812\n",
      "Epoch  672 loss:  0.13404586352407932\n",
      "Epoch  673 loss:  0.13378622569143772\n",
      "Epoch  674 loss:  0.13332874793559313\n",
      "Epoch  675 loss:  0.13284769048914313\n",
      "Epoch  676 loss:  0.13263504998758435\n",
      "Epoch  677 loss:  0.1325531150214374\n",
      "Epoch  678 loss:  0.13232781691476703\n",
      "Epoch  679 loss:  0.1321051176637411\n",
      "Epoch  680 loss:  0.13201531488448381\n",
      "Epoch  681 loss:  0.1318708835169673\n",
      "Epoch  682 loss:  0.1317184162326157\n",
      "Epoch  683 loss:  0.13158022798597813\n",
      "Epoch  684 loss:  0.13151332642883062\n",
      "Epoch  685 loss:  0.13138929102569818\n",
      "Epoch  686 loss:  0.13116975454613566\n",
      "Epoch  687 loss:  0.13105890899896622\n",
      "Epoch  688 loss:  0.13100739987567067\n",
      "Epoch  689 loss:  0.1309150825254619\n",
      "Epoch  690 loss:  0.13081689504906535\n",
      "Epoch  691 loss:  0.13080144068226218\n",
      "Epoch  692 loss:  0.13077986473217607\n",
      "Epoch  693 loss:  0.13074457785114646\n",
      "Epoch  694 loss:  0.13066813815385103\n",
      "Epoch  695 loss:  0.13068984868004918\n",
      "Epoch  696 loss:  0.1306675705127418\n",
      "Epoch  697 loss:  0.13062191288918257\n",
      "Epoch  698 loss:  0.13068710081279278\n",
      "Epoch  699 loss:  0.13077515084296465\n",
      "Epoch  700 loss:  0.130785062443465\n",
      "Epoch  701 loss:  0.130731210578233\n",
      "Epoch  702 loss:  0.13070685975253582\n",
      "Epoch  703 loss:  0.13071181066334248\n",
      "Epoch  704 loss:  0.13069252856075764\n",
      "Epoch  705 loss:  0.13066345918923616\n",
      "Epoch  706 loss:  0.13072618376463652\n",
      "Epoch  707 loss:  0.130820217076689\n",
      "Epoch  708 loss:  0.1307913870550692\n",
      "Epoch  709 loss:  0.13074969872832298\n",
      "Epoch  710 loss:  0.13070432608947158\n",
      "Epoch  711 loss:  0.1306507708504796\n",
      "Epoch  712 loss:  0.1307068932801485\n",
      "Epoch  713 loss:  0.13082428835332394\n",
      "Epoch  714 loss:  0.13079730281606317\n",
      "Epoch  715 loss:  0.1306716096587479\n",
      "Epoch  716 loss:  0.1306516518816352\n",
      "Epoch  717 loss:  0.13057492254301906\n",
      "Epoch  718 loss:  0.13045156840234995\n",
      "Epoch  719 loss:  0.1304731322452426\n",
      "Epoch  720 loss:  0.13046986982226372\n",
      "Epoch  721 loss:  0.1304412316530943\n",
      "Epoch  722 loss:  0.1304135611280799\n",
      "Epoch  723 loss:  0.13038516324013472\n",
      "Epoch  724 loss:  0.1302809678018093\n",
      "Epoch  725 loss:  0.13033181708306074\n",
      "Epoch  726 loss:  0.13046274008229375\n",
      "Epoch  727 loss:  0.13045684108510613\n",
      "Epoch  728 loss:  0.13027320662513375\n",
      "Epoch  729 loss:  0.13017262052744627\n",
      "Epoch  730 loss:  0.13019896764308214\n",
      "Epoch  731 loss:  0.13032248243689537\n",
      "Epoch  732 loss:  0.13027230696752667\n",
      "Epoch  733 loss:  0.13013874972239137\n",
      "Epoch  734 loss:  0.13011160492897034\n",
      "Epoch  735 loss:  0.13016298040747643\n",
      "Epoch  736 loss:  0.13007447170093656\n",
      "Epoch  737 loss:  0.12986187962815166\n",
      "Epoch  738 loss:  0.12983011407777667\n",
      "Epoch  739 loss:  0.12988833105191588\n",
      "Epoch  740 loss:  0.12891894672065973\n",
      "Epoch  741 loss:  0.12770601967349648\n",
      "Epoch  742 loss:  0.1270982315763831\n",
      "Epoch  743 loss:  0.12680079089477658\n",
      "Epoch  744 loss:  0.1264845640398562\n",
      "Epoch  745 loss:  0.1262992904521525\n",
      "Epoch  746 loss:  0.12627740250900388\n",
      "Epoch  747 loss:  0.12615472776815295\n",
      "Epoch  748 loss:  0.12596161430701613\n",
      "Epoch  749 loss:  0.12594234896823764\n",
      "Epoch  750 loss:  0.12597593385726213\n",
      "Epoch  751 loss:  0.12587265251204371\n",
      "Epoch  752 loss:  0.12580416584387422\n",
      "Epoch  753 loss:  0.12578728096559644\n",
      "Epoch  754 loss:  0.1256955759599805\n",
      "Epoch  755 loss:  0.12560055823996663\n",
      "Epoch  756 loss:  0.12554770382121205\n",
      "Epoch  757 loss:  0.12551099946722388\n",
      "Epoch  758 loss:  0.12549544451758265\n",
      "Epoch  759 loss:  0.12550439778715372\n",
      "Epoch  760 loss:  0.12565628020092845\n",
      "Epoch  761 loss:  0.12569440202787519\n",
      "Epoch  762 loss:  0.12563145626336336\n",
      "Epoch  763 loss:  0.12563862325623631\n",
      "Epoch  764 loss:  0.1256977403536439\n",
      "Epoch  765 loss:  0.12561671528965235\n",
      "Epoch  766 loss:  0.12555571179836988\n",
      "Epoch  767 loss:  0.12566366931423545\n",
      "Epoch  768 loss:  0.12563792848959565\n",
      "Epoch  769 loss:  0.12563806911930442\n",
      "Epoch  770 loss:  0.12572697922587395\n",
      "Epoch  771 loss:  0.1257584928534925\n",
      "Epoch  772 loss:  0.12576770409941673\n",
      "Epoch  773 loss:  0.12585731782019138\n",
      "Epoch  774 loss:  0.1260093543678522\n",
      "Epoch  775 loss:  0.12614976847544312\n",
      "Epoch  776 loss:  0.12632563849911094\n",
      "Epoch  777 loss:  0.12641202099621296\n",
      "Epoch  778 loss:  0.1264536790549755\n",
      "Epoch  779 loss:  0.12663472443819046\n",
      "Epoch  780 loss:  0.12694257916882634\n",
      "Epoch  781 loss:  0.1270342725329101\n",
      "Epoch  782 loss:  0.12727435119450092\n",
      "Epoch  783 loss:  0.12759035686030984\n",
      "Epoch  784 loss:  0.12795902835205197\n",
      "Epoch  785 loss:  0.12845730176195502\n",
      "Epoch  786 loss:  0.12889832444489002\n",
      "Epoch  787 loss:  0.1291998941451311\n",
      "Epoch  788 loss:  0.1291161454282701\n",
      "Epoch  789 loss:  0.12875096825882792\n",
      "Epoch  790 loss:  0.1285776225849986\n",
      "Epoch  791 loss:  0.12885414948686957\n",
      "Epoch  792 loss:  0.1288160216063261\n",
      "Epoch  793 loss:  0.12861512647941709\n",
      "Epoch  794 loss:  0.1285999515093863\n",
      "Epoch  795 loss:  0.12804059032350779\n",
      "Epoch  796 loss:  0.12700747279450297\n",
      "Epoch  797 loss:  0.12647322658449411\n",
      "Epoch  798 loss:  0.12621580623090267\n",
      "Epoch  799 loss:  0.12577325385063887\n",
      "Epoch  800 loss:  0.1253631548024714\n",
      "Epoch  801 loss:  0.12513502081856132\n",
      "Epoch  802 loss:  0.12497646128758788\n",
      "Epoch  803 loss:  0.12483699852600694\n",
      "Epoch  804 loss:  0.12495873915031552\n",
      "Epoch  805 loss:  0.12511771637946367\n",
      "Epoch  806 loss:  0.12531014205887914\n",
      "Epoch  807 loss:  0.1255336245521903\n",
      "Epoch  808 loss:  0.1253487216308713\n",
      "Epoch  809 loss:  0.12528713652864099\n",
      "Epoch  810 loss:  0.12564725242555141\n",
      "Epoch  811 loss:  0.1259131096303463\n",
      "Epoch  812 loss:  0.1258486583828926\n",
      "Epoch  813 loss:  0.12576382281258702\n",
      "Epoch  814 loss:  0.12614369206130505\n",
      "Epoch  815 loss:  0.12642199080437422\n",
      "Epoch  817 loss:  0.12608181918039918\n",
      "Epoch  818 loss:  0.12588294222950935\n",
      "Epoch  819 loss:  0.1251990688033402\n",
      "Epoch  820 loss:  0.12488218583166599\n",
      "Epoch  821 loss:  0.12462611775845289\n",
      "Epoch  822 loss:  0.12429730454459786\n",
      "Epoch  823 loss:  0.12398174637928605\n",
      "Epoch  824 loss:  0.12382002780213952\n",
      "Epoch  825 loss:  0.12358269421383739\n",
      "Epoch  826 loss:  0.12334039621055126\n",
      "Epoch  827 loss:  0.12324445182457566\n",
      "Epoch  828 loss:  0.12317386362701654\n",
      "Epoch  829 loss:  0.12298487545922399\n",
      "Epoch  830 loss:  0.1228030794300139\n",
      "Epoch  831 loss:  0.12274514976888895\n",
      "Epoch  832 loss:  0.12267315154895186\n",
      "Epoch  833 loss:  0.12249804893508554\n",
      "Epoch  834 loss:  0.122449800837785\n",
      "Epoch  835 loss:  0.122446499299258\n",
      "Epoch  836 loss:  0.12234912626445293\n",
      "Epoch  837 loss:  0.12224057549610734\n",
      "Epoch  838 loss:  0.1222044201567769\n",
      "Epoch  839 loss:  0.12219515070319176\n",
      "Epoch  840 loss:  0.12213778402656317\n",
      "Epoch  841 loss:  0.12205521948635578\n",
      "Epoch  842 loss:  0.1220193705521524\n",
      "Epoch  843 loss:  0.12201991118490696\n",
      "Epoch  844 loss:  0.12199512729421258\n",
      "Epoch  845 loss:  0.12196521461009979\n",
      "Epoch  846 loss:  0.12192728370428085\n",
      "Epoch  847 loss:  0.12190285976976156\n",
      "Epoch  848 loss:  0.12189298262819648\n",
      "Epoch  849 loss:  0.12187968986108899\n",
      "Epoch  850 loss:  0.12184301065281034\n",
      "Epoch  851 loss:  0.12180652841925621\n",
      "Epoch  852 loss:  0.1218113754875958\n",
      "Epoch  853 loss:  0.12182367825880647\n",
      "Epoch  854 loss:  0.12181672174483538\n",
      "Epoch  855 loss:  0.12180780107155442\n",
      "Epoch  856 loss:  0.12177294492721558\n",
      "Epoch  857 loss:  0.12173194298520684\n",
      "Epoch  858 loss:  0.12173901963979006\n",
      "Epoch  859 loss:  0.12174862064421177\n",
      "Epoch  860 loss:  0.12172922631725669\n",
      "Epoch  861 loss:  0.1216940968297422\n",
      "Epoch  862 loss:  0.12167787970975041\n",
      "Epoch  863 loss:  0.12167772743850946\n",
      "Epoch  864 loss:  0.12167507596313953\n",
      "Epoch  865 loss:  0.12163799721747637\n",
      "Epoch  866 loss:  0.12160797463729978\n",
      "Epoch  867 loss:  0.12160428473725915\n",
      "Epoch  868 loss:  0.12161546712741256\n",
      "Epoch  869 loss:  0.12162046926096082\n",
      "Epoch  870 loss:  0.12161802081391215\n",
      "Epoch  871 loss:  0.12160659674555063\n",
      "Epoch  872 loss:  0.12160881888121367\n",
      "Epoch  873 loss:  0.12163692805916071\n",
      "Epoch  874 loss:  0.12162738712504506\n",
      "Epoch  875 loss:  0.12152103940024972\n",
      "Epoch  876 loss:  0.12130360398441553\n",
      "Epoch  877 loss:  0.12105958396568894\n",
      "Epoch  878 loss:  0.12088116956874728\n",
      "Epoch  879 loss:  0.12052874406799674\n",
      "Epoch  880 loss:  0.12037554755806923\n",
      "Epoch  881 loss:  0.12028253171592951\n",
      "Epoch  882 loss:  0.12023588782176375\n",
      "Epoch  883 loss:  0.12022989336401224\n",
      "Epoch  884 loss:  0.1201394242234528\n",
      "Epoch  885 loss:  0.12010652292519808\n",
      "Epoch  886 loss:  0.12003383785486221\n",
      "Epoch  887 loss:  0.11991354310885072\n",
      "Epoch  888 loss:  0.11981505807489157\n",
      "Epoch  889 loss:  0.11973637202754617\n",
      "Epoch  890 loss:  0.11969411885365844\n",
      "Epoch  891 loss:  0.11972294934093952\n",
      "Epoch  892 loss:  0.11969636613503098\n",
      "Epoch  893 loss:  0.11959799006581306\n",
      "Epoch  894 loss:  0.11958406073972583\n",
      "Epoch  895 loss:  0.1196344057098031\n",
      "Epoch  896 loss:  0.11955381231382489\n",
      "Epoch  897 loss:  0.11947249807417393\n",
      "Epoch  898 loss:  0.11946195084601641\n",
      "Epoch  899 loss:  0.11943541187793016\n",
      "Epoch  900 loss:  0.11934127099812031\n",
      "Epoch  901 loss:  0.11930961441248655\n",
      "Epoch  902 loss:  0.11932619335129857\n",
      "Epoch  903 loss:  0.11929295724257827\n",
      "Epoch  904 loss:  0.11924905935302377\n",
      "Epoch  905 loss:  0.11926114140078425\n",
      "Epoch  906 loss:  0.11932389438152313\n",
      "Epoch  907 loss:  0.11931077763438225\n",
      "Epoch  908 loss:  0.11932694958522916\n",
      "Epoch  909 loss:  0.1193171814084053\n",
      "Epoch  910 loss:  0.11930117756128311\n",
      "Epoch  911 loss:  0.11921853059902787\n",
      "Epoch  912 loss:  0.11920280894264579\n",
      "Epoch  913 loss:  0.119190264493227\n",
      "Epoch  914 loss:  0.11922837235033512\n",
      "Epoch  915 loss:  0.11918554501608014\n",
      "Epoch  916 loss:  0.11922284262254834\n",
      "Epoch  917 loss:  0.1192705212160945\n",
      "Epoch  918 loss:  0.11925342725589871\n",
      "Epoch  919 loss:  0.1192852002568543\n",
      "Epoch  920 loss:  0.11929619638249278\n",
      "Epoch  921 loss:  0.11924748355522752\n",
      "Epoch  922 loss:  0.1192209548316896\n",
      "Epoch  923 loss:  0.11930380342528224\n",
      "Epoch  924 loss:  0.11928689852356911\n",
      "Epoch  925 loss:  0.11931130941957235\n",
      "Epoch  926 loss:  0.11941935867071152\n",
      "Epoch  927 loss:  0.11929761478677392\n",
      "Epoch  928 loss:  0.11934054177254438\n",
      "Epoch  929 loss:  0.11943737789988518\n",
      "Epoch  930 loss:  0.1194321233779192\n",
      "Epoch  931 loss:  0.11945250909775496\n",
      "Epoch  932 loss:  0.11963416589424014\n",
      "Epoch  933 loss:  0.11978536052629352\n",
      "Epoch  934 loss:  0.11980280559509993\n",
      "Epoch  935 loss:  0.11990140751004219\n",
      "Epoch  936 loss:  0.11999260354787111\n",
      "Epoch  937 loss:  0.11992783611640334\n",
      "Epoch  938 loss:  0.11971998075023293\n",
      "Epoch  939 loss:  0.11946930782869458\n",
      "Epoch  940 loss:  0.1189808645285666\n",
      "Epoch  941 loss:  0.11733893258497119\n",
      "Epoch  942 loss:  0.11582608846947551\n",
      "Epoch  943 loss:  0.11510847881436348\n",
      "Epoch  944 loss:  0.11462014634162188\n",
      "Epoch  945 loss:  0.11437074607238173\n",
      "Epoch  946 loss:  0.11413287185132504\n",
      "Epoch  947 loss:  0.11389310145750642\n",
      "Epoch  948 loss:  0.11374057922512293\n",
      "Epoch  949 loss:  0.11353126633912325\n",
      "Epoch  950 loss:  0.11363986972719431\n",
      "Epoch  951 loss:  0.1135363937355578\n",
      "Epoch  952 loss:  0.11352009931579232\n",
      "Epoch  953 loss:  0.11332913534715772\n",
      "Epoch  954 loss:  0.11333940271288157\n",
      "Epoch  955 loss:  0.11321621667593718\n",
      "Epoch  956 loss:  0.1132177640683949\n",
      "Epoch  957 loss:  0.1133643095381558\n",
      "Epoch  958 loss:  0.11353756813332438\n",
      "Epoch  959 loss:  0.11375285033136606\n",
      "Epoch  960 loss:  0.11407196754589677\n",
      "Epoch  961 loss:  0.11384366918355227\n",
      "Epoch  962 loss:  0.11385154956951737\n",
      "Epoch  963 loss:  0.11387388594448566\n",
      "Epoch  964 loss:  0.11382117262110114\n",
      "Epoch  965 loss:  0.11392474407330155\n",
      "Epoch  966 loss:  0.11369586549699306\n",
      "Epoch  967 loss:  0.11340266792103648\n",
      "Epoch  968 loss:  0.11350945336744189\n",
      "Epoch  969 loss:  0.11363064171746373\n",
      "Epoch  970 loss:  0.11317750671878457\n",
      "Epoch  971 loss:  0.11312895221635699\n",
      "Epoch  972 loss:  0.11296934820711613\n",
      "Epoch  973 loss:  0.11263297684490681\n",
      "Epoch  974 loss:  0.11239636270329356\n",
      "Epoch  975 loss:  0.11214127158746123\n",
      "Epoch  976 loss:  0.1120244930498302\n",
      "Epoch  977 loss:  0.11204019701108336\n",
      "Epoch  978 loss:  0.11184020666405559\n",
      "Epoch  979 loss:  0.11154986126348376\n",
      "Epoch  980 loss:  0.11143263289704919\n",
      "Epoch  981 loss:  0.11131158471107483\n",
      "Epoch  982 loss:  0.11119986232370138\n",
      "Epoch  983 loss:  0.11116406600922346\n",
      "Epoch  984 loss:  0.11111784679815173\n",
      "Epoch  985 loss:  0.11115548526868224\n",
      "Epoch  986 loss:  0.111168566159904\n",
      "Epoch  987 loss:  0.1111222100444138\n",
      "Epoch  988 loss:  0.11112889321520925\n",
      "Epoch  989 loss:  0.11117124278098345\n",
      "Epoch  990 loss:  0.11130046425387263\n",
      "Epoch  991 loss:  0.11139170126989484\n",
      "Epoch  992 loss:  0.11148806335404515\n",
      "Epoch  993 loss:  0.11169007793068886\n",
      "Epoch  994 loss:  0.11188523517921567\n",
      "Epoch  995 loss:  0.11203911807388067\n",
      "Epoch  996 loss:  0.11226513981819153\n",
      "Epoch  997 loss:  0.11239280551671982\n",
      "Epoch  998 loss:  0.11271586827933788\n",
      "Epoch  999 loss:  0.1133956890553236\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(init_op)\n",
    "\n",
    "batch=int(X_abs.shape[0]/batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    start = 0\n",
    "    for _ in range(batch):\n",
    "        end = start +batch_size\n",
    "        if end > X_abs.shape[0]:end = X_abs.shape[0]\n",
    "        x_batch = X_abs[start:end]\n",
    "        y_batch = S_abs[start:end]\n",
    "        start = end + 1\n",
    "        _ = sess.run(optimize, feed_dict={x: x_batch, y: y_batch})\n",
    "        c= sess.run(cost, feed_dict={x: x_batch, y: y_batch})\n",
    "        loss += c\n",
    "        \n",
    "    print('Epoch ',epoch, 'loss: ', loss)\n",
    "    \n",
    "    save_path = saver.save(sess, \"/content/gdrive/My Drive/dl02_model.ckpt\")\n",
    "        \n",
    "sess.close()   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-qTcYUowbZ1"
   },
   "source": [
    "# Testing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0gwDbA02j6yJ",
    "outputId": "c87edb7d-af4f-48cc-b9ee-cf9c313fb558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/dl02_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver.restore(sess, \"/content/gdrive/My Drive/dl02_model.ckpt\")\n",
    "First_cleaned = sess.run(y_, feed_dict = {x: np.abs(S1.T)})\n",
    "Second_cleaned = sess.run(y_, feed_dict = {x: np.abs(S2.T)})\n",
    "Original_cleaned=sess.run(y_, feed_dict = {x: np.abs(X.T)})\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VC4TdvWhwifm"
   },
   "source": [
    "# Recover Speech Spectogram from test signal & apply Inverse STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OctUO_zorotP"
   },
   "outputs": [],
   "source": [
    "temp1=np.abs(S1.T)\n",
    "Cleaned1 = (S1/temp1.T)* First_cleaned.T\n",
    "recons_01 = librosa.istft(Cleaned1,  hop_length=512)\n",
    "temp2=np.abs(S2.T)\n",
    "Cleaned2 = (S2/temp2.T)* Second_cleaned.T\n",
    "recons_02 = librosa.istft(Cleaned2, hop_length=512)\n",
    "temp3=np.abs(X.T)\n",
    "CleanedO = (X/temp3.T)* Original_cleaned.T\n",
    "recons_03 = librosa.istft(CleanedO, hop_length=512)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uVr07PWw1cD"
   },
   "source": [
    "# Write and Download the output cleaned output WAV file for test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yI_hVymPly6-"
   },
   "outputs": [],
   "source": [
    "librosa.output.write_wav('test_s_01_recons.wav', recons_01, sr)\n",
    "librosa.output.write_wav('test_s_02_recons.wav', recons_02, sr)\n",
    "librosa.output.write_wav('Original_recons.wav', recons_03, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-00NRyaHuU4"
   },
   "source": [
    "Run Individual Cells to download files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ka07fB7btL48"
   },
   "outputs": [],
   "source": [
    "\n",
    "files.download( \"test_s_01_recons.wav\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgLA7MhkVAi2"
   },
   "outputs": [],
   "source": [
    "\n",
    "files.download( \"test_s_02_recons.wav\" ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xN45gHeSnEr"
   },
   "outputs": [],
   "source": [
    "\n",
    "files.download( \"Original_recons.wav\" ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLQec7k5WehJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL_02_Speech_Denoising.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
